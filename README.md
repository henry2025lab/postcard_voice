# SoundÂ Postcard â€“ Realâ€‘Time Speechâ€‘EmotionÂ Recorder

**A lightweight Python toolkit for recording short speech segments, classifying emotions with the Emotion2Vec+ model, and exporting timeâ€‘stamped trajectories for downstream matching tasks.**

---

## âœ¨ Key Features

| Module                     | Purpose                                                                             | File             |
| -------------------------- | ----------------------------------------------------------------------------------- | ---------------- |
| **AudioRecorder**          | Realâ€‘time microphone capture, noiseâ€‘gate, RMSÂ normalisation, 0.5â€¯s slicing          | `recorder1.py`   |
| **EmotionClassifier**      | Utteranceâ€‘level inference with Emotion2Vec+ Large, optional temperature calibration | `algorithm1.py`  |
| **Temperature Calibrator** | Offline gridâ€‘search of *T* to minimise ECE; saves `temperature.npy`                 | `calibrate_T.py` |
| **GUI (Tk)**               | Start/stop recording, live log, CSV export, oneâ€‘click playback of matched audio     | `gui1.py`        |
| **Main Entrypoint**        | Thread orchestration of recorderÂ â†’ classifierÂ â†’ GUI                                 | `main.py`        |

---

## ğŸ”§ Prerequisites

| Package                                                       | TestedÂ Version |
| ------------------------------------------------------------- | -------------- |
| Python                                                        | Â 3.9Â â€“Â 3.11    |
| PyTorch                                                       | Â â‰¥â€¯2.1         |
| **funasr** (with `iic/emotion2vec_plus_large`)                | Â 1.1.x         |
| NumPy, SoundFile, SoundDevice                                 | latest         |
| Tkinter *(bundled with CPython)*                              | â€“              |
| WindowsÂ â–¶ `winsound` *(builtâ€‘in)* Â· macOS/LinuxÂ â–¶ `playsound` | â€“              |

> **TipÂ ğŸš€**â€ƒGPU is autoâ€‘detected; the code falls back to CPU when `torch.cuda.is_available()` is **False**.

```bash
# 1Â â€“Â create & activate venv (optional but recommended)
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 2Â â€“Â install dependencies
pip install torch sounddevice soundfile numpy playsound funasr
```

---

## â–¶ï¸ QuickÂ Start

```bash
python main.py
```

1. Click **Start Recording** and speak near the mic.
2. EveryÂ 0.5â€¯s, the recognised emotion and confidence are streamed to the log.
3. Click **Stop Recording** to finish.
4. Use **Export toÂ CSV** to save `[timestamp, emotion, score]` for later trajectory matching.
5. Optional: place `matched_audio.wav` beside `gui1.py` â€“ the **PlayÂ MatchedÂ Audio** button will play it asynchronously.

---

## ğŸ“ Calibrating Confidence (TemperatureÂ Scaling)

Run the following once you have a validation set of labelled `.wav` files and an aligned `valid_labels.npy`:

```bash
python calibrate_T.py
```

This script gridâ€‘searches *TÂ âˆˆÂ \[0.3,Â 2.0]* to minimise ExpectedÂ CalibrationÂ Error (ECE), then stores the best scalar in `temperature.npy`. `EmotionClassifier` automatically loads it at startâ€‘up.

---

## ğŸ—‚ï¸ ProjectÂ Tree

```text
â”œâ”€â”€ algorithm1.py      # EmotionClassifier (inference + softmax(T))
â”œâ”€â”€ calibrate_T.py     # offline ECEÂ minimisation
â”œâ”€â”€ gui1.py            # Tk GUI & playback helper
â”œâ”€â”€ main.py            # app bootstrap & threading
â”œâ”€â”€ recorder1.py       # microphone stream & preprocessing
â””â”€â”€ temperature.npy    # (autoâ€‘generated) optimal T
```

---

## ğŸ“„ License

Distributed under the **MIT License** â€“ see `LICENSE` for full text.

> The preâ€‘trained Emotion2Vec+ model is released by the original authors under their own licence; please respect their terms.

---

## ğŸŒ Citing ThisÂ Code

If you use this toolkit in academic work, please cite the corresponding Zenodo release:

```bibtex
@software{li_2025_sound_postcard,
  author       = {HenryÂ Li},
  title        = {SoundÂ Postcard Codebase},
  version      = {1.0.0},
  doi          = {10.5281/zenodo.1234567},
  url          = {https://github.com/henry-li/sound-postcard},
  year         = {2025}
}
```

---

## ğŸ¤ Contributing

PullÂ requests are welcome! Please:

1. ForkÂ â†’Â featureâ€‘branchÂ â†’Â PR.
2. Ensure `flake8` passes and add unit tests where feasible.
3. Describe your change succinctly in `CHANGELOG.md`.

---

## ğŸ’¬ Acknowledgements

This project builds upon the **Emotion2Vec+** modelÂ â€¢ **funasr** toolkitÂ â€¢ **PyTorch** ecosystem. Huge thanks to their respective authors and maintainers.
â€œThis repository relies on the Emotion2Vec+ model released by Z.â€¯Maâ€¯etâ€¯al. under Apacheâ€‘2.0. All rights reserved by the original authors.â€
